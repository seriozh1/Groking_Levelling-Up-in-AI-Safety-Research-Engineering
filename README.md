# AI Safety Research Engineering Progress Tracker

This repository tracks my progress through the [**Levelling Up in AI Safety Research Engineering**](https://www.lesswrong.com/posts/uLstPRyYwzfrx3enG/levelling-up-in-ai-safety-research-engineering) guide by Thomas Woodside, as published on LessWrong. The guide provides a structured, level-based path to develop skills in AI Safety Research Engineering, with objectives, concrete goals, and recommended resources for each level. This README lists all materials from the guide, organized by level, with checkboxes to mark completion.

## Purpose

The goal is to systematically work through the resources and objectives outlined in the guide, documenting my progress in AI Safety Research Engineering. Each level includes specific materials (e.g., courses, books, papers, exercises) that I will complete, with checkboxes to track my advancement.

## Levels and Materials

### Level 1: AI Safety Fundamentals

**Objective**: Learn the basics of why AI Safety is important and decide whether to pursue a theoretical or empirical path.  
**Goals**:  
- Read foundational materials on AI Safety.  
- Engage with arguments for AI Safety to test personal fit.  
- Optionally, start exploring machine learning basics.  

**Resources**:  
- [ ] *Existential Risk from Artificial Intelligence* by Toby Ord (Chapter from *The Precipice*)  
- [ ] *Superintelligence* by Nick Bostrom  
- [ ] *Human Compatible* by Stuart Russell  
- [ ] Alignment Forum posts (e.g., *The Alignment Problem* by Brian Christian, or similar introductory posts)  
- [ ] LessWrong posts on AI Safety (e.g., *AI Safety FAQ* or equivalent introductory articles)  
- [ ] 80,000 Hours AI Safety career profile  

### Level 2: Machine Learning Fundamentals

**Objective**: Gain a solid foundation in machine learning concepts and basic implementation skills.  
**Goals**:  
- Understand core ML concepts (e.g., supervised/unsupervised learning, neural networks).  
- Implement simple ML models using Python and libraries like TensorFlow or PyTorch.  
- Complete introductory ML courses and exercises.  

**Resources**:  
- [ ] *Deep Learning* by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (selected chapters on fundamentals)  
- [ ] Coursera *Machine Learning* by Andrew Ng  
- [ ] Fast.ai *Practical Deep Learning for Coders*  
- [ ] Kaggle introductory ML courses (e.g., *Intro to Machine Learning*, *Intermediate Machine Learning*)  
- [ ] *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow* by Aurélien Géron (selected chapters)  
- [ ] Coding exercises: Implement a simple neural network from scratch  
- [ ] Kaggle competitions (e.g., Titanic dataset, digit recognizer)  

### Level 3: Advanced Machine Learning

**Objective**: Deepen ML knowledge and gain practical experience with advanced models and frameworks.  
**Goals**:  
- Master advanced ML techniques (e.g., CNNs, RNNs, GANs).  
- Work on complex ML projects using frameworks like PyTorch or TensorFlow.  
- Understand optimization, regularization, and model evaluation.  

**Resources**:  
- [ ] *Deep Learning* by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (advanced chapters)  
- [ ] Coursera *Deep Learning Specialization* by Andrew Ng  
- [ ] Fast.ai *Deep Learning from the Foundations*  
- [ ] *Dive into Deep Learning* by Aston Zhang, Zachary C. Lipton, Mu Li, and Alexander J. Smola  
- [ ] PyTorch or TensorFlow tutorials (e.g., official documentation, online tutorials)  
- [ ] Kaggle advanced competitions (e.g., image classification, NLP tasks)  
- [ ] Coding exercises: Implement a CNN or RNN for a specific task (e.g., image classification, text generation)  
- [ ] Blog posts on advanced ML topics (e.g., Distill.pub articles on neural network architectures)  

### Level 4: AI Safety Research Engineering

**Objective**: Apply ML skills to AI Safety problems, understand safety-specific research, and contribute to empirical work.  
**Goals**:  
- Read and summarize AI Safety research papers.  
- Reproduce results from AI Safety experiments.  
- Contribute to open-source AI Safety projects or implement safety-related algorithms.  

**Resources**:  
- [ ] *Post-hoc Interpretability for Neural NLP: A Survey* by Madsen et al.  
- [ ] *Locating and Editing Factual Associations in GPT (ROME)* by Meng et al.  
- [ ] *Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift* by Baek et al.  
- [ ] Alignment Forum posts on interpretability and robustness (e.g., *Mechanistic Interpretability* series)  
- [ ] LessWrong posts on AI Safety engineering (e.g., *Redwood Research* project summaries)  
- [ ] Open-source AI Safety projects (e.g., Redwood Research, Anthropic’s interpretability work)  
- [ ] Coding exercises: Reproduce a simple interpretability experiment (e.g., visualizing attention in a transformer)  
- [ ] Write a blog post summarizing an AI Safety paper or experiment  

### Level 5: Advanced AI Safety Research

**Objective**: Generate novel AI Safety research questions, design experiments, and potentially transition to Research Scientist roles.  
**Goals**:  
- Formulate original research questions in AI Safety.  
- Design and execute empirical experiments to test hypotheses.  
- Apply for AI Safety residencies, Ph.D. programs, or research roles.  

**Resources**:  
- [ ] *Post-hoc Interpretability for Neural NLP: A Survey* by Madsen et al. (revisited for deeper analysis)  
- [ ] *Locating and Editing Factual Associations in GPT (ROME)* by Meng et al. (revisited for experimental design)  
- [ ] *Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift* by Baek et al. (revisited for research question formulation)  
- [ ] Alignment Forum posts on advanced safety topics (e.g., *Scalable Alignment* or *Robustness* series)  
- [ ] LessWrong posts on novel AI Safety research (e.g., *Anthropic’s Research Agenda* summaries)  
- [ ] Research exercises: Design an experiment to test a hypothesis in interpretability or robustness  
- [ ] Write a research proposal or paper draft on an AI Safety topic  
- [ ] Apply to AI Safety residencies (e.g., Anthropic, DeepMind Safety, Redwood Research)  
- [ ] Explore Ph.D. programs in AI Safety or related fields  

## Notes

- **Progress Tracking**: Checkboxes will be marked (`- [x]`) as I complete each resource. Progress updates may be committed to this repository.  
- **Resources**: Some resources (e.g., Alignment Forum posts, LessWrong posts) are broad categories. I’ll select specific, relevant posts and document them in the repository as I progress.  
- **Feedback**: Suggestions for additional resources or improvements to this tracker are welcome via GitHub issues.  

## Acknowledgments

This README is based on the [**Levelling Up in AI Safety Research Engineering**](https://www.lesswrong.com/posts/uLstPRyYwzfrx3enG/levelling-up-in-ai-safety-research-engineering) guide by Thomas Woodside, cross-posted to the EA Forum. Thanks to the author for providing a structured path to skill up in AI Safety Research Engineering.
